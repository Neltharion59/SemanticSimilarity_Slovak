# Library-like script providing data from SemEval competition

# Dictionary of best results (Pearson coefficient) achieved at SemEval competition.
# The values are not for the best system, but best value any system achieved for given dataset.
# This sometimes meant average or worse systems.
semeval_person_results = {
    "dataset_sts_2012_MSRpar_sk.txt": 0.7343,
    "dataset_sts_2012_OnWN_sk.txt": 0.7273,
    "dataset_sts_2012_SMTnews_sk.txt": 0.6085,
    "dataset_sts_2012_SMTeuroparl_sk.txt": 0.5666,

    "dataset_sts_2013_FNWN_sk.txt": 0.5818,
    "dataset_sts_2013_headlines_sk.txt": 0.7838,
    "dataset_sts_2013_OnWN_sk.txt": 0.8431,

    "dataset_sts_2014_deft-forum_sk.txt": 0.5305,
    "dataset_sts_2014_deft-news_sk.txt": 0.7850,
    "dataset_sts_2014_headlines_sk.txt": 0.7666,
    "dataset_sts_2014_images_sk.txt": 0.8214,
    "dataset_sts_2014_OnWN_sk.txt": 0.8589,
    "dataset_sts_2014_tweet-news_sk.txt": 0.7793,

    "dataset_sts_2015_answers-forums_sk.txt": 0.7390,
    "dataset_sts_2015_answers-students_sk.txt": 0.7879,
    "dataset_sts_2015_belief_sk.txt": 0.7717,
    "dataset_sts_2015_headlines_sk.txt": 0.8417,
    "dataset_sts_2015_images_sk.txt": 0.8713,

    "dataset_sts_2016_answer-answer_sk.txt": 0.69235,
    "dataset_sts_2016_headlines_sk.txt": 0.82749,
    "dataset_sts_2016_plagiarism_sk.txt": 0.84138,
    "dataset_sts_2016_postediting_sk.txt": 0.86690,
    "dataset_sts_2016_question-question_sk.txt": 0.74705
}
